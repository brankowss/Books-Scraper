# Book Scraper & Data Pipeline Project

## Overview  

This project is a complete data pipeline. I am using **Scrapy** for scraping, **SQL** for cleaning, modeling, and testing books data from multiple publishers. Then, **Airflow** for workflow orchestration, **PostgreSQL** for data storage, **Docker** for containerization, and **Telegram** bot notifications for monitoring pipeline execution.

![Architecture Diagram](/screenshots/architecture_diagram.png)  

## Key Components  

| Component         | Technology     | Purpose                          |
|-------------------|----------------|----------------------------------|
| Scraping Engine   | Scrapy         | Extract raw books data from publishers |
| Data Lake         | PostgreSQL     | Store raw/cleaned data in `public` schema |
| Data Warehouse    | PostgreSQL     | Modeled data in `dim`/`fact`/`brg` schemas |
| Orchestration     | Airflow        | Pipeline scheduling & monitoring |
| Alerting          | Telegram Bot   | Real-time pipeline notifications |
| Infrastructure    | Docker         | Environment consistency          |

## Project Structure
```
.
‚îú‚îÄ‚îÄ airflow
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dags
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ clean_data.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ model_data.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ scrape_publishers.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ validation_data.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ logs
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ plugins
‚îú‚îÄ‚îÄ booksscraper
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ items.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ scraping_stats.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ settings.py
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ spiders
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ publisher1_spider.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ publisher2_spider.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ publisher3_spider.py
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ modeling
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ brg
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_book_author.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ insert_book_author.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_schemas.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dim
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_authors.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_currency.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_date.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_publishers.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ insert_authors.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ insert_currency.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ insert_date.sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ insert_publishers.sql
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ fact
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ create_book_prices.sql
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ create_books.sql
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ insert_book_prices.sql
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ insert_books.sql
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ scrapy.cfg
‚îú‚îÄ‚îÄ screenshots
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ architecture_diagram.png
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_1.png
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_2.png
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_3.png
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_44.png
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_list.png
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ telegram_alert.jpg
‚îú‚îÄ‚îÄ sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ backup.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ clean_data.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ delete_rename.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ load_cleaned_data.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ normalize_data.sql
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ transform_data.sql
‚îî‚îÄ‚îÄ tests
    ‚îú‚îÄ‚îÄ clean_data_checks.sql
    ‚îú‚îÄ‚îÄ model_checks.sql
    ‚îî‚îÄ‚îÄ raw_data_checks.sql
```

## Database Schema (Modeled Data)  
### Star Schema Design  
#### Dimensions (`dim` schema)  
- **dim.publishers**: Publishing houses  
- **dim.authors**: Book authors (supports many-to-many relationships)  
- **dim.currency**: Currency metadata   
- **dim.date**: Time dimension (2020-2030 for historical analysis)  

#### Facts (`fact` schema)  
- **fact.books**: Core book attributes   
- **fact.books_prices**: Historical price tracking with `is_current` flag  

#### Bridge Tables (`brg` schema)  
- **brg.book_author**: Links books to their authors  

## Setup and Installation  

### 1. Clone the Repository  
```bash  
git clone https://github.com/brankowss/Books-Scraper.git  
cd Books-Scraper  
```

### 2. Configure Environment Variables  
Create `.env` in the project root:
```ini  
POSTGRES_USER=your_postgres_user  
POSTGRES_PASSWORD=your_postgres_password
POSTGRES_DB=your_database_name
POSTGRES_HOST=postgres  # Important: The service name in docker-compose
POSTGRES_PORT=5432
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
AIRFLOW_EXECUTOR=LocalExecutor
AIRFLOW_DATABASE_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
TELEGRAM_BOT_TOKEN=your_bot_token  
TELEGRAM_CHAT_ID=your_chat_id  
```

### 3. Start Containers  
```bash  
docker-compose up --build  
```

### Airflow/Telegram Configuration  
4.  **Configure Airflow and Telegram connections:**

    -   Open the Airflow UI in your browser (usually `http://localhost:8080`).
    -   Go to `Admin` -> `Connections`.
    -   Create a new connection for PostgreSQL with the following details:
        -   `Conn Id`: `postgres_default`
        -   `Conn Type`: `Postgres`
        -   `Host`: `postgres`
        -   `Login`: `your_user_name`
        -   `Password`: `your_password`
        -   `Port`: `5432`
    -   Create a new connection for Telegram with the following details:
        -   `Conn Id`: `telegram_conn`
        -   `Conn Type`: `HTTP`
        -   `Host`: `https://api.telegram.org`
        -   `Login`: `{TELEGRAM_CHAT_ID}`
        -   `Password`: `{TELEGRAM_BOT_TOKEN}`  

    **Ensure you have a Telegram bot set up and configured before proceeding.**

### Airflow DAG üì∏ Screenshots  

### 1Ô∏è‚É£ DAGs List  
Shows all registered DAGs and their statuses.  
![DAGs List](/screenshots/dag_list.png)

### 2Ô∏è‚É£ DAG 1 scrapy_to_postgres
 
![DAG 1 Graph View](/screenshots/dag_1.png)

### 3Ô∏è‚É£ DAG 2 clean_data 

![DAG 2 Graph View](/screenshots/dag_2.png)

### 4Ô∏è‚É£ DAG 3 model_data
 
![DAG 3 Graph View](/screenshots/dag_3.png)

### 5Ô∏è‚É£ DAG 4 validation_data

![DAG 4 Graph View](/screenshots/dag_4.png)


## Real-Time Monitoring  
Receive Telegram alerts for:  
- Pipeline failures  
- Pipeline success

![Telegram Notification](/screenshots/telegram_alert.jpg)  


## Future Improvements  
- **Scalability**: Again(YES) that was a reason why I was build book_prices and date tables.
- **Cloud Integration**: Migrate to AWS.  
- **Dashboard**: Build a dashboard for analytics and much more.  






